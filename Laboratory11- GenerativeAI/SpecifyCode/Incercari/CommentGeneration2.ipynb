{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-07T16:11:54.226322300Z",
     "start_time": "2024-06-07T16:11:44.890043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load your dataset\n",
    "with open('data/gptCodeSnippets.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"code\": [item['code'] for item in data],\n",
    "    \"specifications\": [item['specifications'] for item in data]\n",
    "})\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"facebook/bart-base\"  # You can choose any model suitable for code generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples['code']\n",
    "    targets = examples['specifications']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "# \n",
    "# # Training arguments\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=3,\n",
    "#     predict_with_generate=True\n",
    "# )\n",
    "# \n",
    "# # Trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_data,\n",
    "#     eval_dataset=tokenized_data,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# \n",
    "# # Train the model\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:653: FutureWarning: `is_compiling` is deprecated. Use `torch.compiler.is_compiling()` instead.\n",
      "  return dynamo.is_compiling()\n",
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:448: FutureWarning: `is_compiling` is deprecated. Use `torch.compiler.is_compiling()` instead.\n",
      "  or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n",
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:369: FutureWarning: `is_compiling` is deprecated. Use `torch.compiler.is_compiling()` instead.\n",
      "  or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n",
      "C:\\Users\\Deea\\Desktop\\Uni\\By Year\\Second year\\Semester 2\\AI\\Laboratories\\Laboratory_12\\venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:259: FutureWarning: `is_compiling` is deprecated. Use `torch.compiler.is_compiling()` instead.\n",
      "  or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def quick_sort(arr): #    if len(arr) <= 1:   d   I   return arr[len(arr))   o   pivot = arr [len(r) // 2]   e   left = [x for x in arr if x < pivot] Â   right = [X for x for x if x == pivot] [x in arr)   i   pivot = (x for y in arr, if x > pivot) Ã   middle = [y for x i in arrif x > piv] _______________________________  _____________________   ______   �   x   =\n"
     ]
    }
   ],
   "source": [
    "def generate_specifications(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
    "    specification = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return specification\n",
    "\n",
    "# Example usage\n",
    "new_code = \"\"\"def quick_sort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[len(arr) // 2]\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quick_sort(left) + middle + quick_sort(right)\"\"\"\n",
    "generated_specification = generate_specifications(new_code)\n",
    "print(generated_specification)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T16:12:03.145856Z",
     "start_time": "2024-06-07T16:11:56.915126800Z"
    }
   },
   "id": "f1c15eb7d3395776"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c30bb6b144a8eea8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
